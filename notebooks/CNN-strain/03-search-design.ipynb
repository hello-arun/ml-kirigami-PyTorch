{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZG149sjfrvS",
        "outputId": "501abe12-6c61-4773-a6ce-0edd91ba40ec"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader,Subset,ConcatDataset\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "PROJECT_DIR = \"../../\"\n",
        "RESULT_DIR = \"./results-search-design/\"\n",
        "os.makedirs(RESULT_DIR,exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ejWJTvRsUIz"
      },
      "source": [
        "## Helper class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OJHFWAjkhHj0"
      },
      "outputs": [],
      "source": [
        "class Helper:\n",
        "    def make_fine_grid(self,raw_data, n_corse_x = 3, n_corse_y = 5, n_fine_x = 30, n_fine_y = 80):\n",
        "        listFG = []  # List Fine Grid\n",
        "        N = len(raw_data)\n",
        "        for i in range(N):\n",
        "            print(f\"Pre Processing {i+1:05d}/{N}, {100*(i+1)//N}%\",end=\"\\r\")\n",
        "            kirigami_config = raw_data[i, 0:15]\n",
        "            inner_wCuts = self.corse_to_fine_config(\n",
        "                kirigami_config, n_corse_x, n_corse_y, n_fine_x, n_fine_y)\n",
        "            listFG.append(inner_wCuts)\n",
        "\n",
        "        alldata_FG = np.array(listFG)\n",
        "        alldata_FG = np.append(alldata_FG, raw_data[:, -3:], 1)\n",
        "        return alldata_FG\n",
        "\n",
        "    def corse_to_fine_config(self,kirigami_config, n_corse_x, n_corse_y, n_fine_x, n_fine_y):\n",
        "        \"\"\"\n",
        "        Make Fine Grid using corse grid\n",
        "        0 5 10     0  80  160 ... 2320\n",
        "        1 6 11     .  .    .  ... .\n",
        "        2 7 12  => .  .    .  ... .\n",
        "        3 8 13     .  .    .  ... .\n",
        "        4 9 14     79 159 239 ... 2399\n",
        "\n",
        "        Parameters\n",
        "        --------------------\n",
        "        kirigami_config: Corse Kirigami config of size n_corse_x * n_corse_y\n",
        "        return: Fine grid 1D array of size n_fine_x*n_fine_y\n",
        "        \"\"\"\n",
        "        fine_grid = np.ones((n_fine_x,n_fine_y))\n",
        "        mx, my = n_fine_x//n_corse_x, n_fine_y//n_corse_y  # 10 16\n",
        "        zeros = np.array([1]*mx)[:,np.newaxis]\n",
        "        zeros[mx//3:2*mx//3+1]=0\n",
        "        # ONLY MAKE CUTS inside the INNER REGION !!\n",
        "        for index,num in enumerate(kirigami_config):\n",
        "            if num == 0:\n",
        "                i_corse_x = index // n_corse_y\n",
        "                i_corse_y = index % n_corse_y\n",
        "                fine_grid[mx*i_corse_x:mx*(i_corse_x+1),my*i_corse_y:my*(i_corse_y+1)] = zeros\n",
        "        return fine_grid.flatten()\n",
        "helper = Helper()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S70acHFspzh"
      },
      "source": [
        "## Prepare Dataset\n",
        "We conver coarse grid data to fine grid data using helper class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dGaSVmR3gI56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre Processing 29791/29791, 100%\r"
          ]
        }
      ],
      "source": [
        "alldata_15G = np.loadtxt(f'{PROJECT_DIR}/raw/alldata_15G.dat',dtype=np.float32)\n",
        "alldata_FG = helper.make_fine_grid(alldata_15G)\n",
        "del alldata_15G\n",
        "del helper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeOarrgEtfVi"
      },
      "source": [
        "## Data Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC46CrU4tlWM",
        "outputId": "137e0969-375e-48cd-a0aa-8c4a3df1abcd"
      },
      "outputs": [],
      "source": [
        "FEATURES = len(alldata_FG[0])-3\n",
        "STRAIN, TOUGHNESS, STRESS = -3, -2, -1\n",
        "prop = STRAIN\n",
        "max_prop = np.max(alldata_FG[:, prop])\n",
        "inputs = torch.from_numpy(alldata_FG[:, 0:FEATURES]).float().reshape(-1,1,30,80)\n",
        "targets = torch.from_numpy(alldata_FG[:, prop]/max_prop).float().reshape(-1,1)\n",
        "dataset = TensorDataset(inputs,targets)\n",
        "del inputs,targets,alldata_FG\n",
        "# print(targets.dtype,inputs.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training and searching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KirigamiModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # output: 16 x 15 x 40\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # output: 32 x 7 x 20\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # output: 64 x 3 x 10\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1920,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,1),\n",
        "        )\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        images, targets = batch\n",
        "        preds = self(images)                  # Generate predictions\n",
        "        loss = F.mse_loss(preds, targets)  # Calculate loss\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        images, targets = batch\n",
        "        preds = self(images)                    # Generate predictions\n",
        "        loss = F.mse_loss(preds, targets)       # Calculate loss\n",
        "        # acc = accuracy(preds, targets)        # Calculate accuracy\n",
        "        return {'val_loss': loss.detach()}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        # batch_accs = [x['val_acc'] for x in outputs]\n",
        "        # epoch_acc = torch.stack(batch_accs).mean()    # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item()}\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}\".format(\n",
        "            epoch, result['train_loss'], result['val_loss']))\n",
        "# trial_input = torch.rand((4,1,30,80))\n",
        "# print(trial_input.dtype)\n",
        "# model(trial_input).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.Adam):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in train_loader:\n",
        "            imgs, targets = batch\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device,non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_samples = 100  # Number of initial samples\n",
        "num_generations = 10 # Numbers of generations to run to find optimal designs\n",
        "\n",
        "train_indicies = range(0,100)\n",
        "test_indicies = range(100,len(dataset))\n",
        "train_ds_full = Subset(dataset,range(0,100))\n",
        "test_ds = Subset(dataset,range(100,len(dataset)))\n",
        "plot_data = np.empty((0,2))\n",
        "gen_id=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_size = int(0.9 * len(train_ds_full))\n",
        "val_size = len(train_ds_full) - train_size\n",
        "train_ds, val_ds = random_split(train_ds_full,[train_size,val_size])\n",
        "train_dl = DataLoader(train_ds,batch_size=50,shuffle=True,pin_memory=True)\n",
        "val_dl = DataLoader(val_ds,batch_size=10,pin_memory=True)\n",
        "test_dl = DataLoader(test_ds,batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = get_default_device()\n",
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "val_dl = DeviceDataLoader(val_dl, device)\n",
        "test_dl = DeviceDataLoader(test_dl, device)\n",
        "model = to_device(KirigamiModel(), device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0], train_loss: 0.0656, val_loss: 0.0528\n",
            "Epoch [1], train_loss: 0.0361, val_loss: 0.0447\n",
            "Epoch [2], train_loss: 0.0329, val_loss: 0.0430\n",
            "Epoch [3], train_loss: 0.0309, val_loss: 0.0388\n",
            "Epoch [4], train_loss: 0.0293, val_loss: 0.0370\n",
            "Epoch [5], train_loss: 0.0265, val_loss: 0.0320\n",
            "Epoch [6], train_loss: 0.0256, val_loss: 0.0292\n",
            "Epoch [7], train_loss: 0.0226, val_loss: 0.0321\n",
            "Epoch [8], train_loss: 0.0239, val_loss: 0.0238\n",
            "Epoch [9], train_loss: 0.0211, val_loss: 0.0233\n",
            "Epoch [10], train_loss: 0.0203, val_loss: 0.0211\n",
            "Epoch [11], train_loss: 0.0171, val_loss: 0.0179\n",
            "Epoch [12], train_loss: 0.0154, val_loss: 0.0155\n",
            "Epoch [13], train_loss: 0.0138, val_loss: 0.0139\n",
            "Epoch [14], train_loss: 0.0118, val_loss: 0.0124\n"
          ]
        }
      ],
      "source": [
        "history =[]\n",
        "history += fit(15, 0.001, model, train_dl, val_dl, torch.optim.Adam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29091\n",
            "29091 700\n"
          ]
        }
      ],
      "source": [
        "preds = torch.tensor([])\n",
        "for batch in test_dl:\n",
        "    imgs,targets = batch \n",
        "    preds =  torch.cat((preds,model(imgs).detach().flatten().cpu()),dim=0)\n",
        "print(len(preds))\n",
        "\n",
        "reals = torch.tensor([])\n",
        "for batch in train_dl:\n",
        "    _,targets = batch \n",
        "    reals =  torch.cat((reals,targets.flatten().cpu()),dim=0)\n",
        "for batch in val_dl:\n",
        "    _,targets = batch \n",
        "    reals =  torch.cat((reals,targets.flatten().cpu()),dim=0)\n",
        "   \n",
        "print(len(preds),len(reals))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "800\n",
            "28991\n"
          ]
        }
      ],
      "source": [
        "y_pred=preds.numpy()\n",
        "\n",
        "max,_= torch.topk(reals,100)\n",
        "max=torch.mean(max).item()\n",
        "plot_data = np.append(plot_data,[[gen_id,max]],axis=0)\n",
        "\n",
        "sample_indices = np.argpartition(y_pred,-num_samples)[-num_samples:]  # Best Performers\n",
        "\n",
        "new_train_ds = Subset(test_ds,sample_indices)\n",
        "print(len(new_train_ds))\n",
        "train_ds_full = ConcatDataset((train_ds_full,new_train_ds))\n",
        "\n",
        "mask=torch.ones(len(test_ds))\n",
        "mask[sample_indices] = 0\n",
        "test_ds = Subset(test_ds,torch.where(mask == 1)[0])\n",
        "\n",
        "print(len(train_ds_full))\n",
        "print(len(test_ds))\n",
        "gen_id+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.cla()\n",
        "plt.plot(plot_data[:,0],plot_data[:,1])\n",
        "plt.xlim(0,10)\n",
        "plt.ylim(0,1)\n",
        "plt.savefig(\"./temp.png\",facecolor=\"White\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.makedirs(\"./models/regression\",exist_ok=True)\n",
        "# torch.save(model.state_dict(), './models/regression/f16-f32-f64-h64.pth')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9S70acHFspzh"
      ],
      "name": "CNN-regression.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit ('pytorch-1.12.0')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "4bf133e68ea69dc128944cff77d5d532fddfd436c4ee2509df098c607a2dfdac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
