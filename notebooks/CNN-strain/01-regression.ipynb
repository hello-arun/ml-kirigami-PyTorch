{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZG149sjfrvS",
        "outputId": "501abe12-6c61-4773-a6ce-0edd91ba40ec"
      },
      "outputs": [],
      "source": [
        "# %tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib\n",
        "# matplotlib.use('TKAgg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "import os\n",
        "PROJECT_DIR = \"../../\"\n",
        "os.makedirs(\"./results/\",exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 648x252 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, axes = plt.subplots(1, 3)\n",
        "fig.set_facecolor(\"White\")\n",
        "fig.set_size_inches(9, 3.5)\n",
        "\n",
        "# fig.tight_layout(pad=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reset_fig():\n",
        "    for ax in axes:\n",
        "        ax.cla()\n",
        "        ax.set_xlim([0, 1])\n",
        "        ax.set_ylim([0, 1])\n",
        "        ax.set_xlabel(\"Predicted\")\n",
        "        ax.set_ylabel(\"Target\")\n",
        "        # ratio = 1.0\n",
        "        # x_left, x_right = ax.get_xlim()\n",
        "        # y_low, y_high = ax.get_ylim()\n",
        "        # ax.set_aspect(\"equal\")\n",
        "    axes[0].set_title(\"Train\")\n",
        "    axes[1].set_title(\"Validation\")\n",
        "    axes[2].set_title(\"Test\")\n",
        "    fig.suptitle(\"Hello\")\n",
        "    fig.subplots_adjust(\n",
        "        left=0.075,\n",
        "        right=0.975,\n",
        "        top=0.85,\n",
        "        bottom=0.15,\n",
        "        wspace=0.3, hspace=0)\n",
        "    fig.savefig(\"temp.png\", dpi=300)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ejWJTvRsUIz"
      },
      "source": [
        "## Helper class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OJHFWAjkhHj0"
      },
      "outputs": [],
      "source": [
        "class Helper:\n",
        "    def make_fine_grid(self,raw_data, n_corse_x = 3, n_corse_y = 5, n_fine_x = 30, n_fine_y = 80):\n",
        "        listFG = []  # List Fine Grid\n",
        "        N = len(raw_data)\n",
        "        for i in range(N):\n",
        "            print(f\"Pre Processing {i+1:05d}/{N}, {100*(i+1)//N}%\",end=\"\\r\")\n",
        "            kirigami_config = raw_data[i, 0:15]\n",
        "            inner_wCuts = self.corse_to_fine_config(\n",
        "                kirigami_config, n_corse_x, n_corse_y, n_fine_x, n_fine_y)\n",
        "            listFG.append(inner_wCuts)\n",
        "\n",
        "        alldata_FG = np.array(listFG)\n",
        "        alldata_FG = np.append(alldata_FG, raw_data[:, -3:], 1)\n",
        "        return alldata_FG\n",
        "\n",
        "    def corse_to_fine_config(self,kirigami_config, n_corse_x, n_corse_y, n_fine_x, n_fine_y):\n",
        "        \"\"\"\n",
        "        Make Fine Grid using corse grid\n",
        "        0 5 10     0  80  160 ... 2320\n",
        "        1 6 11     .  .    .  ... .\n",
        "        2 7 12  => .  .    .  ... .\n",
        "        3 8 13     .  .    .  ... .\n",
        "        4 9 14     79 159 239 ... 2399\n",
        "\n",
        "        Parameters\n",
        "        --------------------\n",
        "        kirigami_config: Corse Kirigami config of size n_corse_x * n_corse_y\n",
        "        return: Fine grid 1D array of size n_fine_x*n_fine_y\n",
        "        \"\"\"\n",
        "        fine_grid = np.ones((n_fine_x,n_fine_y))\n",
        "        mx, my = n_fine_x//n_corse_x, n_fine_y//n_corse_y  # 10 16\n",
        "        zeros = np.array([1]*mx)[:,np.newaxis]\n",
        "        zeros[mx//3:2*mx//3+1]=0\n",
        "        # ONLY MAKE CUTS inside the INNER REGION !!\n",
        "        for index,num in enumerate(kirigami_config):\n",
        "            if num == 0:\n",
        "                i_corse_x = index // n_corse_y\n",
        "                i_corse_y = index % n_corse_y\n",
        "                fine_grid[mx*i_corse_x:mx*(i_corse_x+1),my*i_corse_y:my*(i_corse_y+1)] = zeros\n",
        "        return fine_grid.flatten()\n",
        "helper = Helper()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S70acHFspzh"
      },
      "source": [
        "## Prepare Dataset\n",
        "We conver coarse grid data to fine grid data using helper class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dGaSVmR3gI56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre Processing 29791/29791, 100%\r"
          ]
        }
      ],
      "source": [
        "alldata_15G = np.loadtxt(f'{PROJECT_DIR}/raw/alldata_15G.dat',dtype=np.float32)\n",
        "alldata_FG = helper.make_fine_grid(alldata_15G)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeOarrgEtfVi"
      },
      "source": [
        "## Split Train, validation, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC46CrU4tlWM",
        "outputId": "137e0969-375e-48cd-a0aa-8c4a3df1abcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.float32 torch.float32\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(23833, 2979, 2979)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FIG_NO=0\n",
        "FEATURES = 2400\n",
        "STRAIN, TOUGHNESS, STRESS = -3, -2, 1\n",
        "prop=STRAIN\n",
        "max_prop = np.max(alldata_FG[:, STRAIN])\n",
        "\n",
        "inputs = torch.from_numpy(alldata_FG[:, 0:FEATURES]).float().reshape(-1,1,30,80)\n",
        "targets = torch.from_numpy(alldata_FG[:, prop]/max_prop).float().reshape(-1,1)\n",
        "print(targets.dtype,inputs.dtype)\n",
        "dataset = TensorDataset(inputs, targets)\n",
        "train_ds, val_ds, test_ds = random_split(dataset, [23833, 2979, 2979])\n",
        "len(train_ds), len(val_ds), len(test_ds)\n",
        "# Shuffle the data although not needed as it is already suffled\n",
        "# np.random.shuffle(alldata_FG)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataLoader and Batch Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 200\n",
        "train_dl = DataLoader(train_ds,batch_size,shuffle=True,pin_memory=True)\n",
        "val_dl = DataLoader(val_ds,batch_size,pin_memory=True)\n",
        "test_dl = DataLoader(test_ds,batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cuxttlngu3Kh"
      },
      "source": [
        "## Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KirigamiModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # output: 16 x 15 x 40\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # output: 32 x 7 x 20\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # output: 64 x 3 x 10\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1920,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,1),\n",
        "        )\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        images, targets = batch\n",
        "        preds = self(images)                  # Generate predictions\n",
        "        loss = F.mse_loss(preds, targets)  # Calculate loss\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        images, targets = batch\n",
        "        preds = self(images)                    # Generate predictions\n",
        "        loss = F.mse_loss(preds, targets)       # Calculate loss\n",
        "        # acc = accuracy(preds, targets)        # Calculate accuracy\n",
        "        return {'val_loss': loss.detach()}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        # batch_accs = [x['val_acc'] for x in outputs]\n",
        "        # epoch_acc = torch.stack(batch_accs).mean()    # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item()}\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}\".format(\n",
        "            epoch, result['train_loss'], result['val_loss']))\n",
        "\n",
        "model = KirigamiModel()\n",
        "# trial_input = torch.rand((4,1,30,80))\n",
        "# print(trial_input.dtype)\n",
        "# model(trial_input).shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device,non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = get_default_device()\n",
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "val_dl = DeviceDataLoader(val_dl, device)\n",
        "test_dl = DeviceDataLoader(test_dl, device)\n",
        "model = to_device(model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.Adam):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in train_loader:\n",
        "            imgs, targets = batch\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "opt_func = torch.optim.Adam\n",
        "lr = 0.001\n",
        "history=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0], train_loss: 0.0048, val_loss: 0.0035\n",
            "Epoch [0], train_loss: 0.0035, val_loss: 0.0026\n",
            "Epoch [0], train_loss: 0.0024, val_loss: 0.0025\n",
            "Epoch [0], train_loss: 0.0017, val_loss: 0.0014\n",
            "Epoch [0], train_loss: 0.0016, val_loss: 0.0011\n",
            "Epoch [0], train_loss: 0.0014, val_loss: 0.0010\n",
            "Epoch [0], train_loss: 0.0010, val_loss: 0.0009\n",
            "Epoch [0], train_loss: 0.0009, val_loss: 0.0009\n",
            "Epoch [0], train_loss: 0.0009, val_loss: 0.0009\n",
            "Epoch [0], train_loss: 0.0008, val_loss: 0.0008\n",
            "Epoch [0], train_loss: 0.0008, val_loss: 0.0008\n",
            "Epoch [0], train_loss: 0.0008, val_loss: 0.0008\n",
            "Epoch [0], train_loss: 0.0007, val_loss: 0.0007\n",
            "Epoch [0], train_loss: 0.0007, val_loss: 0.0007\n",
            "Epoch [0], train_loss: 0.0007, val_loss: 0.0010\n",
            "Epoch [0], train_loss: 0.0007, val_loss: 0.0008\n",
            "Epoch [0], train_loss: 0.0006, val_loss: 0.0007\n",
            "Epoch [0], train_loss: 0.0006, val_loss: 0.0007\n",
            "Epoch [0], train_loss: 0.0006, val_loss: 0.0007\n",
            "Epoch [0], train_loss: 0.0006, val_loss: 0.0007\n",
            "Epoch [0], train_loss: 0.0006, val_loss: 0.0007\n",
            "Epoch [0], train_loss: 0.0006, val_loss: 0.0007\n",
            "Epoch [0], train_loss: 0.0006, val_loss: 0.0007\n",
            "Epoch [0], train_loss: 0.0006, val_loss: 0.0007\n",
            "Epoch [0], train_loss: 0.0006, val_loss: 0.0008\n",
            "Epoch [0], train_loss: 0.0006, val_loss: 0.0006\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\jangi\\Documents\\github\\ml-kirigami-PyTorch\\notebooks\\CNN-strain\\01-regression.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jangi/Documents/github/ml-kirigami-PyTorch/notebooks/CNN-strain/01-regression.ipynb#ch0000033?line=2'>3</a>\u001b[0m reset_fig()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jangi/Documents/github/ml-kirigami-PyTorch/notebooks/CNN-strain/01-regression.ipynb#ch0000033?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m imgs,targets \u001b[39min\u001b[39;00m train_dl:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jangi/Documents/github/ml-kirigami-PyTorch/notebooks/CNN-strain/01-regression.ipynb#ch0000033?line=5'>6</a>\u001b[0m     axes[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mplot(model(imgs)\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu(),targets\u001b[39m.\u001b[39;49mcpu(),\u001b[39m\"\u001b[39;49m\u001b[39mk.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jangi/Documents/github/ml-kirigami-PyTorch/notebooks/CNN-strain/01-regression.ipynb#ch0000033?line=6'>7</a>\u001b[0m     axes[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mplot([\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m],[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m],\u001b[39m\"\u001b[39m\u001b[39mr--\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jangi/Documents/github/ml-kirigami-PyTorch/notebooks/CNN-strain/01-regression.ipynb#ch0000033?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m imgs,targets \u001b[39min\u001b[39;00m val_dl:\n",
            "File \u001b[1;32mc:\\Users\\jangi\\Miniconda3\\envs\\pytorch-1.12.0\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1634\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1632\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[0;32m   1633\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m-> 1634\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_line(line)\n\u001b[0;32m   1635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request_autoscale_view(scalex\u001b[39m=\u001b[39mscalex, scaley\u001b[39m=\u001b[39mscaley)\n\u001b[0;32m   1636\u001b[0m \u001b[39mreturn\u001b[39;00m lines\n",
            "File \u001b[1;32mc:\\Users\\jangi\\Miniconda3\\envs\\pytorch-1.12.0\\lib\\site-packages\\matplotlib\\axes\\_base.py:2283\u001b[0m, in \u001b[0;36m_AxesBase.add_line\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   2280\u001b[0m \u001b[39mif\u001b[39;00m line\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2281\u001b[0m     line\u001b[39m.\u001b[39mset_clip_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch)\n\u001b[1;32m-> 2283\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_line_limits(line)\n\u001b[0;32m   2284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39mget_label():\n\u001b[0;32m   2285\u001b[0m     line\u001b[39m.\u001b[39mset_label(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_child\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_children)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\jangi\\Miniconda3\\envs\\pytorch-1.12.0\\lib\\site-packages\\matplotlib\\axes\\_base.py:2306\u001b[0m, in \u001b[0;36m_AxesBase._update_line_limits\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   2302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_line_limits\u001b[39m(\u001b[39mself\u001b[39m, line):\n\u001b[0;32m   2303\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2304\u001b[0m \u001b[39m    Figures out the data limit of the given line, updating self.dataLim.\u001b[39;00m\n\u001b[0;32m   2305\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2306\u001b[0m     path \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39;49mget_path()\n\u001b[0;32m   2307\u001b[0m     \u001b[39mif\u001b[39;00m path\u001b[39m.\u001b[39mvertices\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2308\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\jangi\\Miniconda3\\envs\\pytorch-1.12.0\\lib\\site-packages\\matplotlib\\lines.py:999\u001b[0m, in \u001b[0;36mLine2D.get_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[39m\"\"\"Return the `~matplotlib.path.Path` associated with this line.\"\"\"\u001b[39;00m\n\u001b[0;32m    998\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_invalidy \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_invalidx:\n\u001b[1;32m--> 999\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecache()\n\u001b[0;32m   1000\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path\n",
            "File \u001b[1;32mc:\\Users\\jangi\\Miniconda3\\envs\\pytorch-1.12.0\\lib\\site-packages\\matplotlib\\lines.py:656\u001b[0m, in \u001b[0;36mLine2D.recache\u001b[1;34m(self, always)\u001b[0m\n\u001b[0;32m    654\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_x\n\u001b[0;32m    655\u001b[0m \u001b[39mif\u001b[39;00m always \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_invalidy:\n\u001b[1;32m--> 656\u001b[0m     yconv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_yunits(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_yorig)\n\u001b[0;32m    657\u001b[0m     y \u001b[39m=\u001b[39m _to_unmasked_float_array(yconv)\u001b[39m.\u001b[39mravel()\n\u001b[0;32m    658\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\jangi\\Miniconda3\\envs\\pytorch-1.12.0\\lib\\site-packages\\matplotlib\\artist.py:264\u001b[0m, in \u001b[0;36mArtist.convert_yunits\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39mif\u001b[39;00m ax \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m ax\u001b[39m.\u001b[39myaxis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    263\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n\u001b[1;32m--> 264\u001b[0m \u001b[39mreturn\u001b[39;00m ax\u001b[39m.\u001b[39;49myaxis\u001b[39m.\u001b[39;49mconvert_units(y)\n",
            "File \u001b[1;32mc:\\Users\\jangi\\Miniconda3\\envs\\pytorch-1.12.0\\lib\\site-packages\\matplotlib\\axis.py:1497\u001b[0m, in \u001b[0;36mAxis.convert_units\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1495\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_units\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m   1496\u001b[0m     \u001b[39m# If x is natively supported by Matplotlib, doesn't need converting\u001b[39;00m\n\u001b[1;32m-> 1497\u001b[0m     \u001b[39mif\u001b[39;00m munits\u001b[39m.\u001b[39;49m_is_natively_supported(x):\n\u001b[0;32m   1498\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m   1500\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconverter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\jangi\\Miniconda3\\envs\\pytorch-1.12.0\\lib\\site-packages\\matplotlib\\units.py:62\u001b[0m, in \u001b[0;36m_is_natively_supported\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mReturn whether *x* is of a type that Matplotlib natively supports or an\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39marray of objects of such types.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39m# Matplotlib natively supports all number types except Decimal.\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39;49miterable(x):\n\u001b[0;32m     63\u001b[0m     \u001b[39m# Assume lists are homogeneous as other functions in unit system.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[39mfor\u001b[39;00m thisx \u001b[39min\u001b[39;00m x:\n\u001b[0;32m     65\u001b[0m         \u001b[39mif\u001b[39;00m thisx \u001b[39mis\u001b[39;00m ma\u001b[39m.\u001b[39mmasked:\n",
            "File \u001b[1;32mc:\\Users\\jangi\\Miniconda3\\envs\\pytorch-1.12.0\\lib\\site-packages\\numpy\\lib\\function_base.py:385\u001b[0m, in \u001b[0;36miterable\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[39mCheck whether or not an object can be iterated over.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m \n\u001b[0;32m    383\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 385\u001b[0m     \u001b[39miter\u001b[39;49m(y)\n\u001b[0;32m    386\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\jangi\\Miniconda3\\envs\\pytorch-1.12.0\\lib\\site-packages\\torch\\_tensor.py:729\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state():\n\u001b[0;32m    725\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m'\u001b[39m\u001b[39mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    726\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mPassing a tensor of different shape won\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mt change the number of \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    727\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39miterations executed (and might lead to errors or silently give \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    728\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mincorrect results).\u001b[39m\u001b[39m'\u001b[39m, category\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mTracerWarning, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munbind(\u001b[39m0\u001b[39;49m))\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for i in range(num_epochs):\n",
        "    history = fit(1, lr, model, train_dl, val_dl, opt_func)\n",
        "    reset_fig()\n",
        "\n",
        "    for imgs,targets in train_dl:\n",
        "        axes[0].plot(model(imgs).detach().cpu(),targets.cpu(),\"k.\")\n",
        "        axes[0].plot([0,1],[0,1],\"r--\")\n",
        "\n",
        "    for imgs,targets in val_dl:\n",
        "        axes[1].plot(model(imgs).detach().cpu(),targets.cpu(),\"k.\")\n",
        "        axes[1].plot([0,1],[0,1],\"r--\")\n",
        "\n",
        "\n",
        "    for imgs,targets in test_dl:\n",
        "        axes[2].plot(model(imgs).detach().cpu(),targets.cpu(),\"k.\")\n",
        "        axes[2].plot([0,1],[0,1],\"r--\")\n",
        "    fig.suptitle(f\"Epoch: {FIG_NO:02d}\")\n",
        "    fig.savefig(F\"./results/epoch-{FIG_NO:02d}.png\",dpi=300)\n",
        "    FIG_NO+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"./models/regression\",exist_ok=True)\n",
        "torch.save(model.state_dict(), './models/regression/f16-f32-f64-h64.pth')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9S70acHFspzh"
      ],
      "name": "CNN-regression.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit ('pytorch-1.12.0')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "4bf133e68ea69dc128944cff77d5d532fddfd436c4ee2509df098c607a2dfdac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
